<section id="MLprojects">
  <h2>Machine Learning Projects</h2>
    <article class="project-card">
      &nbsp;&nbsp;<a href="#Movie-Rec">1. Recommendation System with Movielens Dataset</a><br>
      &nbsp;&nbsp;<a href="#NLP">2. Sentiment Analysis with Amazon Customer Reviews Dataset</a><br>
      &nbsp;&nbsp;<a href="#Marketing-Campaign">3. A/B Testing with Marketing Campaign Dataset</a><br>
      &nbsp;&nbsp;<a href="#Churn">4. Customer Churn Prediction with Telco Customer Dataset</a><br>
      &nbsp;&nbsp;<a href="#SQL">5. Retail E-commerce SQL Analytics</a>
      <br><br>
<!------------------------------------------------------>
      <!-- PROJECT 1) MOVIELENS 25M RECOMMENDATION -->
      <section id="Movie-Rec">
        <header class="project-header">
          <h3>
            Movie Recommendations: From Matrix Factorization to Hybrid Ranking
            <span class="year">2025</span>
            <!-- Github link -->
            <div class="links">
              <a class="btn" href="https://github.com/Yeonji-Ji/movielens-25m-recsys" target="_blank" rel="noopener">GitHub</a>
            </div>
          </h3>
          <br>
          <!-- Key words -->
          <div class="tags">
          <span style="background-color:#f6cbd7"> Matrix Factorization </span> <span style="background-color:#d2f8e4"> SVD </span>
          <span style="background-color:#f6cbd7"> Hybrid Ranking </span> <span style="background-color:#d2f8e4"> LightGBM </span>
          <span style="background-color:#f6cbd7"> Feature Engineering </span> <span style="background-color:#d2f8e4"> EDA </span>
          <span style="background-color:#f6cbd7"> Precision </span> <span style="background-color:#d2f8e4"> Recall </span>
          <span style="background-color:#f6cbd7"> NDCG </span> <span style="background-color:#d2f8e4"> Recommendation </span>
          </div>
          <br>
          <!-- Pipeline Figure -->
          <figure class="project-figure">
            <img src="image/diagram/Movielens25M.drawio.png" 
                alt="Movie RecSys pipeline: Raw Data → Feature Engineering → MF & Hybrid Ranker → Evaluation"
                style="width: 100%; height: auto;"  />
            <figcaption>Movie Recommendation System pipeline</figcaption>
          </figure>
          <br>
          <!-- Project Summry -->
          <ul class="bullets">
            <li><strong>Goal:</strong> Generate personalized movie recommendation lists based on movie features and customer preferences</li>
            <li><strong>Data:</strong> MovieLens rating (25,000,095) data combined with movie metadata (genres, popularity, quality, recency)</li>
            <li><strong>Approach:</strong>
              <ul>
                <li><strong>EDA & Preprocessing:</strong> Filter ratings after 2015 and apply 10-core filtering</li>
                <li><strong>Feature Engineering:</strong> Built user and item features including genres, popularity (#ratings), quality (mean rating), and recency</li>
                <li><strong>Baseline:</strong> Applied Matrix Factorization (SVD) to learn user & item embeddings and generate Top-K candidates</li>
                <li><strong>Hybrid Model:</strong> Combined MF scores with content-based features and trained a ranking model using LightGBM</li>
              </ul>
            </li>
            <li><strong>Evaluation:</strong> Assessed models using <em>Precision</em>, <em>Recall</em>, and <em>NDCG</em> metrics</li>
            <li><strong>Outcome:</strong> The hybrid model achieved modest but consistent gains over the MF baseline in several key metrics (Precision@10/20, Recall@5/10/20, NDCG@20)</li>
          </ul>

          <p>
            <strong>Business Insight: </strong><br>
            By combining features like popularity, genres and recency, the hybrid model captures both individual tastes and trending content, 
            helping platforms keep recommendations both personalized and timely.
          </p>
          <figure class="project-figure">
          <img src="image/movielens-25m-recsys/movielens25m-fig_metrics_mf_vs_hybrid.png" alt="MF vs. Hybrid" style="width:100%; height:auto;">
          <figcaption>Monthly Revenue Trend</figcaption>
          </figure>
        </header>
      </section>
      <br><br>

<!------------------------------------------------------>
      <!-- PROJECT 2) AMAZON REVIEW SENTIMENT -->
      <section id="NLP">
        <header class="project-header">
          <h3>
            Sentiment Analysis of Amazon Customer Reviews using TF-IDF
            <span class="year">2025</span>
            <!-- Github link -->
            <div class="links">
              <a class="btn" href="https://github.com/Yeonji-Ji/amazon-reviews-tfidf-sentiment" target="_blank" rel="noopener">GitHub</a>
            </div>
          </h3>
          <br>
          <!-- Key words -->
          <div class="tags">
          <span style="background-color:#f6cbd7"> NLP </span> <span style="background-color:#d2f8e4"> Sentiment Analysis </span>
          <span style="background-color:#f6cbd7"> Reviews </span> <span style="background-color:#d2f8e4"> TF-IDF </span>
          <span style="background-color:#f6cbd7"> Logistic Regression </span> <span style="background-color:#d2f8e4"> Naive Bayes </span>
          <span style="background-color:#f6cbd7"> SVM </span> <span style="background-color:#d2f8e4"> LinearSVC </span>
          <span style="background-color:#f6cbd7"> XGBoost </span>
          </div>
          <br>
          <!-- Pipeline Figure -->
          <figure class="project-figure">
            <img src="image/diagram/Amazon Review Sentiment.drawio.png" 
                alt="Sentiment Analysis pipeline: Raw Data → EDA/Preprocessing -> TF-IDF → Model → Evaluation"
                style="width: 100%; height: auto;"  />
            <figcaption>Sentiment Analysis Pipeline</figcaption>
          </figure>
          <br>
          <!-- Project Summry -->
          <ul class="bullets">
            <li><strong>Goal:</strong> Develop a machine learning model to classify Amazon customer reviews as positive or negative.</li>
            <li><strong>Data:</strong> Millions of Amazon customer reviews (text) with star ratings as sentiment labels.</li>
            <li><strong>Approach:</strong>
              <ul>
                <li><strong>EDA & Preprocessing:</strong> Explored class distribution, text length, and frequent words (via WordCloud).</li>
                    <figure class="project-figure">
                    <img src="image/amazon-reviews-sentiment/WordCloud.png" alt="Positive vs. Negative" style="width:80%; height:auto;">
                    <figcaption>WordCloud of Positive vs. Negative Reviews</figcaption>
                    </figure>
                <li><strong>Feature Extraction:</strong> Applied TF-IDF to capture word importance.</li>
                <li><strong>Linear Models:</strong> Trained Logistic Regression and Naive Bayes (Multinomial/Complement).</li>
                <li><strong>Tree-Based Model:</strong> Evaluated XGBoost.</li>
              </ul>
            </li>
            <li><strong>Evaluation:</strong> Compared models using <em>Accuracy</em>, <em>Precision</em>, <em>Recall</em>, <em>F1-score</em>, <em>ROC-AUC</em>, and <em>Average Precision</em>.</li>
            <li><strong>Outcome:</strong> Logistic Regression, Naive Bayes, and Linear SVC delivered strong, reliable baselines for text classification. In contrast, XGBoost underperformed, showing that more complex models are not always superior.</li>
                <figure class="project-figure">
                <img src="image/amazon-reviews-sentiment/ConfusionMatrix_LR_NB.png" alt="CM of LR vs. NB" style="width:100%; height:auto;">
                <figcaption>Confusion Matrix of Logistic Regression (LR), Multinomial Naive Bayes (Multi_NB), Complement Naive Bayes (Comp_NB)</figcaption>
                </figure>

          </ul>

          <p>
            <strong>Business Insight: </strong><br>
            The high-dimensional, sparse feature space generated by TF-IDF favors linear classifiers over tree-based methods like XGBoost. 
            This highlights the importance of selecting algorithms aligned with data characteristics, rather than defaulting to complex models.
          </p>

          <details class="more">
            <summary><span style="background-color:#C0FFFF">Show Results (Logistic Regression, Naive Bayes, LinearSVC, XGBoost)</span></summary>
              <h4>Logistic Regression vs. Naive Bayes:</h4>

              <figure class="project-figure">
              <img src="image/amazon-reviews-sentiment/ROC_AUC_PR_AP.png" alt="LR vs. NB" style="width:80%; height:auto;">
              <figcaption>ROC Curve & Precision-Recall Curve of Logistic Regression (LR), Multinomial Naive Bayes (Multi_NB), Complement Naive Bayes (Comp_NB)</figcaption>
              </figure>

              <h4>Support Vector Machins (LinearSVC):</h4>
              <figure class="project-figure">
              <img src="image/amazon-reviews-sentiment/LinearSVC_ROC_AUC_PR_AP.png" alt="LinearSVC" style="width:80%; height:auto;">
              <figcaption>ROC Curve & Precision-Recall Curve of LinearSVC</figcaption>
              </figure>

              <h4>Tree-Based Model (XGBoost):</h4>
              <figure class="project-figure">
              <img src="image/amazon-reviews-sentiment/XGBoost_ROC_AUC_PR_AP.png" alt="XGBoost" style="width:80%; height:auto;">
              <figcaption>ROC Curve & Precision-Recall Curve of XGBoost</figcaption>
              </figure>            
          </details>
        </header>
      </section>
      <br><br>

<!------------------------------------------------------>
      <!-- PROJECT 3) Marketing Campaign -->
      <section id="Marketing-Campaign">
        <header class="project-header">
          <h3>
            Marketing Campaign Effect
            <span class="year">2025</span>
            <!-- Github link -->
            <div class="links">
              <a class="btn" href="https://github.com/Yeonji-Ji/movielens-25m-recsys" target="_blank" rel="noopener">GitHub</a>
            </div>
            <div class="links">
              <a class="btn" href="https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign" target="_blank" rel="noopener">Dataset</a>
            </div>
          </h3>
          <br>
          <!-- Key words -->
          <div class="tags">
          <span style="background-color:#f6cbd7"> Welch's t-test </span> <span style="background-color:#d2f8e4"> Bayesian A/B Test </span>
          <span style="background-color:#f6cbd7"> Posterior Inference </span> <span style="background-color:#d2f8e4"> PSM </span>
          <span style="background-color:#f6cbd7"> ATT </span> <span style="background-color:#d2f8e4"> SMD </span>
          <span style="background-color:#f6cbd7"> Feature Engineering </span> 
          </div>
          <br>

          <!-- Project Summry -->
          <ul class="bullets">
            <li><strong>Goal:</strong> Measure and validate the true effectiveness of a marketing campaign beyond raw response counts.</li>
            <li><strong>Data:</strong> 
              <ul>
                <li>Customer demographics: age, income, household with kids, etc.</li>
                <li>Marketing history: prior campaign acceptance (aggregated features)</li>
                <li>Spending behavior: Total Spend (key performance indicator)</li>
              </ul>
            </li>

          <!-- Data Figure -->
          <figure class="project-figure">
            <img src="image/marketing-campaigns/box_plot_totalspend_response.png" 
                alt="Comparison"
                style="width: 50%; height: auto;"  />
            <figcaption>Total Spend by Campaign Response</figcaption>
          </figure>
          <br>

            <li><strong>Method:</strong>
              <ul>
                <li>Feature Engineering: Created response features via one-hot encoding & aggregation</li>
                <li>Statistical Testing: Conducted Welch’s t-test, validated consistency with a Bayesian A/B test</li>
                <li>Causal Inference: Applied Propensity Score Matching (11 covariates, 334 matched pairs)</li>
                <ul>
                  <li>Explored Caliper adjustment</li>
                  <li>Compared methodologies: Logistic Regression vs. XGBoost</li>
                </ul>
                <li>Covariate Balance: Evaluated with Standardized Mean Differences (SMD) and proposed refinements</li>
                <li>Uplift Modeling: Estimated CATE (Conditional Average Treatment Effect) to identify persuadable customers</li>
              </ul>
            </li>
            <li><strong>Results:</strong>
              <ul>
                <li><strong>Statistical Tests:</strong>
                  <ul>
                    <li>Significant campaign effect: t = 10.87, p ≈ 0.0</li>
                    <li>Bayesian posterior estimates aligned with frequentist results → confirms robustness</li>
                    <li>ATT = 228.76 after controlling for confounders</li>
                  </ul>
                </li>
                <li><strong>Propensity Score Matching (SMD):</strong>
                  <ul>
                    <li>Identified high-imbalance features (e.g., AcceptedCnt)</li>
                    <li>SMD improvements:</li>
                    <ul>
                      <li>Base: 0.186 -> With Caliper: 0.168 -> With XGBoost: 0.141</li>
                    </ul>
                    <li>Some features remain hard to match</li>
                    <li>SMD Before & After (with XGBoost): Figures in <span style="background-color:#C0FFFF">Show Results</span></li>
                  </ul>
                </li>
                <li><strong>Uplift Model:</strong> Figures in <span style="background-color:#C0FFFF">Show Results</span>
                  <ul>
                    <li>Predicted CATE scores (Top 5):  [-72.47, -9.18, 43.36, 5.91, 53.13]</li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><strong>Key Takeaways</strong>
              <ul>
                <li>Campaign demonstrated statistical and causal effectiveness.</li>
                <li>PSM with caliper and advanced models improved balance, though challenges remained for certain features.</li>
                <li>Uplift modeling provided insights into customer heterogeneity, highlighting which customers are most persuadable.</li>
              </ul>
            </li>

          <details class="more">
            <summary><span style="background-color:#C0FFFF">Show Results</span></summary>
              <h4>Bayesian A/B Testing:</h4>
              <figure class="project-figure">
              <img src="image/marketing-campaigns/posterior_distribution_B_over_A.png" alt="B over A" style="width:65%; height:auto;">
              <figcaption>Posterior Distribution of the Difference</figcaption>
              </figure>

              <h4>Propensity Score:</h4>
              <figure class="project-figure">
              <img src="image/marketing-campaigns/propensity_score_distribution.png" alt="Propensity Score" style="width:80%; height:auto;">
              <figcaption>Propensity Score Distribution Before and After Matching</figcaption>
              </figure>
              
              <h4>PSM-SMD:</h4>
              <figure class="project-figure">
              <img src="image/marketing-campaigns/covariate_balance_before_after_PSM.png" alt="SMD" style="width:75%; height:auto;">
              <figcaption>SMD of Covariates Before & After PSM</figcaption>
              </figure>

              <h4>PSM-SMD with XGBoost:</h4>
              <figure class="project-figure">
              <img src="image/marketing-campaigns/smd_xgboost.png" alt="SMD" style="width:75%; height:auto;">
              <figcaption>SMD of Covariates Before & After PSM with XGBoost</figcaption>
              </figure>

              <h4>Distribution of Predicted CATE Scores:</h4>
              <p>
              Customers with positive uplift (Persuadables) are clear on the right tail, while negative uplift (Sleeping Dogs) appear on the left. 
              This highlights which segments benefit from targeting and which should be excluded.
              </p>
              <figure class="project-figure">
              <img src="image/marketing-campaigns/predicted_CATE_distribution.png" alt="SMD" style="width:65%; height:auto;">
              <figcaption>Distribution of Predicted CATE Scores</figcaption>
              </figure>

              <h4>Cumulative Uplift Curve:</h4>
              <p>
              Targeting the top 10–30% of customers ranked by uplift score yields significantly higher gains compared to random targeting (e.g., ~746 vs. ~450). 
              Beyond ~60%, returns diminish and performance can fall below random, suggesting optimal targeting thresholds.
              </p>
              <figure class="project-figure">
              <img src="image/marketing-campaigns/Uplift_Curve.png" alt="SMD" style="width:65%; height:auto;">
              <figcaption>Cumulative Uplift Curve</figcaption>
              </figure>
          </details>
      </section>
      <br><br>

<!------------------------------------------------------>
      <!-- Telco: 상세 카드 (비즈니스 맥락 + 시각 요소 + 세부 내용) -->
      <section id="Churn">
        <header class="project-header">
          <h3>
            Telco Customer Churn Prediction
            <span class="year">2025</span>
            <!-- Github link -->
            <div class="links">
              <a class="btn" href="https://github.com/Yeonji-Ji/telco-customer-churn-prediction" target="_blank" rel="noopener">GitHub</a>
            </div>
          </h3>
          <br>
          <!-- Key words -->
          <div class="tags">
            <span style="background-color:#f6cbd7"> Churn </span> <span style="background-color:#d2f8e4"> Imbalanced Data </span>
            <span style="background-color:#f6cbd7"> SMOTE </span> <span style="background-color:#d2f8e4"> XGBoost </span>
            <span style="background-color:#f6cbd7"> Logistic Regression </span> <span style="background-color:#d2f8e4"> Random Forest </span>
            <span style="background-color:#f6cbd7"> LightGBM </span> <span style="background-color:#d2f8e4"> Feature Engineering </span>
          </div>

        <!-- 핵심 요약 (비즈니스 임팩트 중심) -->
          <ul class="bullets">
              <li><strong>Goal:</strong>Develop predictive models to identify telecom customers at risk of churn, 
                enabling proactive retention strategies and improved customer lifetime value.
              </li>
              <li><strong>Data:</strong> 
                <ul>
                  <li>Customer demographics: gender, age range, partner, dependents</li>
                  <li>Customer account: tenure, contract type, payment method, billing, charges</li>
                  <li>Services subscribed: phone, internet, online security, backup, device protection, tech support, streaming services</li>
                  <li>Churn: whether the customer left in the last month</li>
                </ul>
              </li>

              <figure class="project-figure">
                <img src="image/telco_churn/churn_vs_n.png" alt="churn" style="width:75%; height:auto;">
                <figcaption>Churn Distribution</figcaption>
              </figure>

              <li><strong>Method:</strong>
                <ul>
                  <li>Exploratory Data Analysis: Identified tenure, billing, and service usage as strong churn drivers</li>
                  <li>Baseline Model: Logistic Regression used as benchmark</li>
                  <li>Class Imbalance Handling: Applied class weights and SMOTE oversampling</li>
                  <li>Tree-based Models: Random Forest, XGBoost, LightGBM for stronger recall–precision trade-off</li>
                  <li>Evaluation Metrics: Accuracy, Precision, Recall, F1-score (focus on churn class)</li>
                </ul>
              </li>

              <li><strong>Results:</strong>
                <ul>
                  <li>Logistic Regression: Interpretable baseline but limited recall on churners</li>
                  <li>Balanced Logistic Regression: Improved recall, but with reduced precision</li>
                  <li>Random Forest: Delivered the best balance across precision and recall</li>
                  <li>XGBoost / LightGBM: Strong alternatives with competitive performance</li>
                </ul>
              </li>

              <li><strong>Key Takeaways:</strong>
              <ul>
                <li>Random Forest provided the most balanced churn prediction, making it suitable for production deployment.</li>
                <li>High-recall models like XGBoost are valuable when the priority is catching at-risk customers, even at the cost of false positives.</li>
                <li>Proactive churn management strategies can be informed by these models, enabling retention campaigns focused on high-risk segments.</li>
              </ul>
            </li>
          </ul>

          <details class="more">
            <summary><span style="background-color:#C0FFFF">Show Results</span></summary>
              <div data-include="data/telco_results_table.html"></div>
              <!-- <script src="js/table.js"></script> -->

              <figure class="project-figure">
                <img src="image/telco_churn/Baseline_Model_CM.png" alt="LogReg" style="width:90%; height:auto;">
                <figcaption>Confusion Matrix of Baseline Logistic Regression Model</figcaption>
              </figure>

              <figure class="project-figure">
                <img src="image/telco_churn/RandomForest.png" alt="RF" style="width:40%; height:auto;">
                <figcaption>Confusion Matrix of Random Forest Model</figcaption>
              </figure>

              <figure class="project-figure">
                <img src="image/telco_churn/XGBoost.png" alt="XGBoost" style="width:40%; height:auto;">
                <figcaption>Confusion Matrix of XGBoost Model</figcaption>
              </figure>

              <figure class="project-figure">
                <img src="image/telco_churn/LightGBM.png" alt="GBM" style="width:40%; height:auto;">
                <figcaption>Confusion Matrix of LightGBM Model</figcaption>
              </figure>

          </details>
        </header>
      </section>
      <br><br>

<!------------------------------------------------------>
      <!-- PROJECT 3) SQL DATA E-COMMERCE -->
      <section id="SQL">
        <header class="project-header">
          <h3>
            Retail E-commerce SQL Analytics
            <span class="year">2025</span>
            <!-- Github link -->
            <div class="links">
              <a class="btn" href="https://github.com/Yeonji-Ji/sql-retail-analytics" target="_blank" rel="noopener">GitHub</a>
            </div>
          </h3>
          <br>
          <!-- Key words -->
          <div class="tags">
            <span style="background-color:#f6cbd7"> SQL </span> <span style="background-color:#d2f8e4"> SQLite </span>
            <span style="background-color:#f6cbd7"> Data Cleaning </span> <span style="background-color:#d2f8e4"> Visualization </span>
          </div>
          <br>
        <!-- Project Summry -->
          <ul class="bullets">
            <li><strong>Goal:</strong> Designed a reproducible SQL analytics pipeline on UK Online Retail data</li>
            <li><strong>Approach:</strong> Built clean data, monthly revenue dashboards, Top-N customer/product insights, and new vs. returning customer retention analysis</li>
            <li><strong>Outcome:</strong>
              <ul>
                <li>Strong <strong>seasonality</strong> with Q4 spikes</li>
                <li><strong>Pareto effect</strong>: Top ~10% customers generate ~60% revenue</li>
                <li>Returning customers drive consistent revenue, highlighting <strong>retention</strong> as a key business driver</li>
                <li>Certain countries outperform, suggesing <strong>geography targeted marketing</strong> potential</li>
              </ul>
            </li>
          </ul>
          <details class="more">
            <summary><span style="background-color:#C0FFFF">Show Results</span></summary>
            <figure class="project-figure">
              <img src="image/sql-ecommerce/sql-ecommerce-fig_monthly_revenue.png" alt="Monthly Revenue Trend" style="max-width:450px; width:100%; height:auto;">
              <figcaption>Monthly Revenue Trend</figcaption>
            </figure>

            <figure class="project-figure">
              <img src="image/sql-ecommerce/sql-ecommerce-fig_new_vs_return.png" alt="New vs Returning Customers" style="max-width:450px; width:100%; height:auto;">
              <figcaption>New vs Returning Customers</figcaption>
            </figure>

            <figure class="project-figure">
              <img src="image/sql-ecommerce/sql-ecommerce-fig_top_customers.png" alt="Top 10 Customers" style="max-width:450px; width:100%; height:auto;">
              <figcaption>Top 10 Customers</figcaption>
            </figure>
          </details>
        </header>
      </section>

<!------------------------------------------------------>
    </article>
</section>